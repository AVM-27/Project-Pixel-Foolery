# ğŸ§  Project Pixel Foolery: Fooling Neural Networks with Few-Pixel Attacks

![Status](https://img.shields.io/badge/status-complete-success)
![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![Frameworks](https://img.shields.io/badge/Frameworks-TensorFlow%20%7C%20Keras-orange)
![License](https://img.shields.io/badge/License-MIT-green)

This project explores the fascinating vulnerability of deep convolutional neural networks (CNNs) to adversarial attacks at the pixel level. We demonstrate that it's possible to fool well-trained models by perturbing just **1 to 5 pixels** in an image, forcing them to misclassify with high confidence.

---

## ğŸ‘¥ The Research Team

This project was a collaborative effort by:
* **Anjana C V** `[CB.EN.U4ECE22003]`
* **Adarsh Venugopal** `[CB.EN.U4ECE22004]`
* **Harini K** `[CB.EN.U4ECE22026]`
* **Vikas P K** `[CB.EN.U4ECE22237]`

---

## ğŸ¯ Project Goal

The primary objective is to perform **black-box adversarial attacks** on pre-trained CNN models (**PureCNN** and **Modified ResNet**) using **Differential Evolution (DE)**. The goal is to alter a minimal number of pixels (1-5) to achieve two outcomes:
1. Drastically reduce the model's confidence in the true class.
2. Induce a confident misclassification into a different class.

---

## ğŸ”¬ Methodology

Our attack strategy is entirely **black-box**, meaning we do not require any knowledge of the model's internal architecture, parameters, or gradients. We use **Differential Evolution (DE)**, a powerful population-based optimization algorithm, to find the optimal pixel perturbations.

The DE algorithm iteratively refines a population of candidate solutions (pixel perturbations) through three main steps:
1. **ğŸ§¬ Mutation:** Creates new candidate solutions by adding weighted differences between existing solutions, introducing variation.
2. **ğŸ”„ Crossover:** Combines parameters from the mutated candidates and the original population to increase diversity.
3. **ğŸ† Selection:** Compares the new candidates to the original ones and keeps only those that are more "fit"â€”in our case, those that most successfully reduce the true class's prediction confidence.

---

## ğŸ§ª Experimental Results

We tested our attack on models trained on the **CIFAR-10** dataset and a binary subset (**Cat vs. Dog**). The results clearly show the models' vulnerability.

### ğŸ”¹ Pre-Updation (CIFAR-10 Multi-Class)

| Model | Epochs | Accuracy (%) | Pixels Perturbed | True Confidence (Before â†’ After) | Misclassified As | Misclass. Confidence | DE Iterations |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| **PureCNN** | 100 | 84.63 | 5 | 0.42 â†’ 0.16 | `Deer` | 0.54 | 100 |
| **Modified ResNet** | 50 | 81.69 | 5 | 0.99 â†’ 0.22 | `Dog` | 0.77 | 200 |

### ğŸ”¸ Post-Updation (Modified ResNet works with only Dog and Cat Classes)

The attacks on the updated ResNet model were exceptionally successful, causing confident misclassifications with incredibly few pixels.

| Pixels Changed | True Class Confidence (Cat) | Predicted Class | Predicted Confidence |
| :---: | :---: | :---: | :---: |
| **1 pixel** | 0.82 â†’ **0.05** | `Dog` | **0.95** |
| **2 pixels** | 0.82 â†’ **0.02** | `Dog` | **0.98** |
| **3 pixels** | 0.82 â†’ **0.00** | `Dog` | **1.00** |
| **4 pixels** | 0.82 â†’ **0.00** | `Dog` | **1.00** |
| **5 pixels** | 0.82 â†’ **0.00** | `Dog` | **1.00** |

#### Visual Result (Example with 5 pixels)
![5 Pixel Attack](RESULTS/After%20Updation/5_Pixels.jpg)
*An original image of a cat is misclassified as a dog with 100% confidence after altering just 5 pixels.*

---

## ğŸ§± Model Architectures

### ğŸ“¦ PureCNN

A custom-built CNN with multiple convolutional and dropout layers, designed for CIFAR-10.  
**Architecture:**
Input â†’ Conv(96) â†’ Dropout â†’ Conv(96) â†’ Conv(96, s=2) â†’ Dropout â†’ Conv(192) â†’ Conv(192) â†’ Conv(192, s=2) â†’ Dropout â†’ Conv(192) â†’ ReLU â†’ Conv(192) â†’ ReLU â†’ Conv(10) â†’ GlobalAvgPool â†’ Softmax

### ğŸ” Modified ResNet

A ResNet-style architecture with residual blocks (skip connections) for improved training stability and performance.  
**Architecture:**
Input â†’ Conv(16) â†’ BN â†’ ReLU  
â†“  
[ResBlock(16)] Ã— 2  
â†“  
[ResBlock(32, s=2)] Ã— 2  
â†“  
[ResBlock(64, s=2)] Ã— 2  
â†“  
AvgPool â†’ Flatten â†’ Dense(10) â†’ Softmax

---

## ğŸ“ Repository Structure

```
Project-Pixel-Foolery/
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ Notebooks/
â”‚   â”œâ”€â”€ PURECNN.ipynb
â”‚   â”œâ”€â”€ RESNET.ipynb
â”‚   â””â”€â”€ WITH_ONLY_2_CLASSES_CAT_AND_DOG.ipynb
â”œâ”€â”€ RESULTS/
â”‚   â”œâ”€â”€ After Updation/
â”‚   â”‚   â”œâ”€â”€ 1_Pixel.jpg ... 5_Pixels.jpg
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ PureCNN-MultiClass.png
â”‚   â”œâ”€â”€ RESNET-MultiClass.png
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ REPORTS/
â”‚   â”œâ”€â”€ BEFORE UPDATION.pdf
â”‚   â”œâ”€â”€ UPDATED REPORT.pdf
â”‚   â””â”€â”€ README.md
â””â”€â”€ 2 Classes/
    â”œâ”€â”€ WITH_ONLY_2_CLASSES_CAT_AND_DOG.ipynb
    â””â”€â”€ README.md
```

---

## âš™ï¸ How to Run

To replicate our experiments, follow these steps:

1. **Clone the repository**
    ```bash
    git clone https://github.com/AVM-27/Project-Pixel-Foolery.git
    cd Project-Pixel-Foolery
    ```

2. **Install dependencies**  
    It is recommended to use a virtual environment.
    ```bash
    pip install -r requirements.txt 
    # (Note: A requirements.txt file should include tensorflow, numpy, matplotlib, etc.)
    ```

3. **Explore the notebooks**  
    Navigate to the `Notebooks/` or `2 Classes/` directory and run the Jupyter Notebooks to see the attack implementation and results.
    ```bash
    jupyter notebook Notebooks/RESNET.ipynb
    ```

---

## ğŸ“œ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.
